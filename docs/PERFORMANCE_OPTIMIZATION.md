# Руководство по оптимизации производительности

## Обзор

AI Agent включает комплексную систему оптимизации производительности, которая автоматически адаптируется к различным нагрузкам и типам документов. Система включает:

- **Многоуровневое кэширование** запросов и эмбеддингов
- **Оптимизированное чанкинг** в зависимости от типа документа
- **Асинхронную обработку** больших документов
- **Мониторинг ресурсов** в реальном времени
- **Автоматическую оптимизацию** параметров

## Архитектура производительности

```
┌─────────────────┐    ┌──────────────────┐    ┌─────────────────┐
│   Query Cache   │    │  Embedding Cache │    │  Async Processor│
│   (LRU + TTL)   │    │   (LRU + TTL)    │    │  (Thread Pool)  │
└─────────────────┘    └──────────────────┘    └─────────────────┘
         │                       │                       │
         └───────────────────────┼───────────────────────┘
                                 │
         ┌───────────────────────┼───────────────────────┐
         │                       │                       │
┌─────────────────┐    ┌──────────────────┐    ┌─────────────────┐
│ Chunk Optimizer │    │Performance Monitor│    │ Health Monitor  │
│ (Type-Aware)    │    │ (Metrics + Stats) │    │ (Resources)     │
└─────────────────┘    └──────────────────┘    └─────────────────┘
```

## Компоненты оптимизации

### 1. Система кэширования

#### Query Cache

- **Назначение**: Кэширование результатов поиска по документам
- **Алгоритм**: LRU (Least Recently Used) с TTL
- **Размер по умолчанию**: 500 записей
- **TTL по умолчанию**: 30 минут

#### Embedding Cache

- **Назначение**: Кэширование векторных представлений текста
- **Алгоритм**: LRU с TTL
- **Размер по умолчанию**: 1000 записей
- **TTL по умолчанию**: 2 часа

#### Конфигурация кэша

```env
# Размеры кэша
CACHE_QUERY_MAX_SIZE=500
CACHE_EMBEDDING_MAX_SIZE=1000

# Время жизни (в секундах)
CACHE_DEFAULT_TTL=1800
CACHE_EMBEDDING_TTL=7200

# Автоматическая очистка
CACHE_CLEANUP_INTERVAL=300  # 5 минут
```

### 2. Оптимизированное чанкинг

Система автоматически определяет тип документа и применяет оптимальную стратегию разбиения:

#### Типы документов

| Тип               | Размер чанка | Перекрытие | Особенности                 |
| ----------------- | ------------ | ---------- | --------------------------- |
| Юридический       | 600-800      | 150        | Сохранение структуры статей |
| Технический       | 1000-1500    | 200        | Большие чанки для контекста |
| Структурированный | 500-600      | 100        | Сохранение списков и таблиц |
| Повествовательный | 800-1000     | 200        | Разбиение по предложениям   |
| Смешанный         | 800-900      | 180        | Балансированный подход      |

#### Алгоритм классификации

```python
# Паттерны для определения типа документа
legal_patterns = [
    r'\b(статья|пункт|подпункт|глава|раздел)\s+\d+',
    r'\b(закон|кодекс|постановление|приказ|регламент)',
    r'\b(договор|контракт|соглашение|протокол)'
]

technical_patterns = [
    r'\b(технические требования|спецификация|стандарт)',
    r'\b(параметры|характеристики|показатели)',
    r'\b(ГОСТ|ТУ|СНиП|СП)\s*\d+'
]
```

### 3. Асинхронная обработка

#### Когда используется

- Документы размером > 50KB (настраивается)
- Пакетная загрузка документов
- Операции с высокой вычислительной нагрузкой

#### Архитектура

- **Thread Pool**: Пул рабочих потоков
- **Task Queue**: Очередь задач с приоритетами
- **Progress Tracking**: Отслеживание прогресса выполнения

#### Конфигурация

```env
# Количество рабочих потоков
ASYNC_MAX_WORKERS=4

# Порог для асинхронной обработки (в символах)
ASYNC_LARGE_DOC_THRESHOLD=50000

# Таймаут обработки (в секундах)
ASYNC_PROCESSING_TIMEOUT=300
```

### 4. Мониторинг производительности

#### Метрики

- **Время выполнения операций**
- **Использование памяти**
- **Коэффициент попаданий в кэш**
- **Количество медленных операций**
- **Загрузка системных ресурсов**

#### Пороги предупреждений

```env
# Медленные операции (в секундах)
PERFORMANCE_SLOW_THRESHOLD=5.0

# Использование памяти (в МБ)
PERFORMANCE_MEMORY_THRESHOLD=500

# CPU (в процентах)
HEALTH_CPU_WARNING_THRESHOLD=80
HEALTH_CPU_CRITICAL_THRESHOLD=95

# Память (в процентах)
HEALTH_MEMORY_WARNING_THRESHOLD=80
HEALTH_MEMORY_CRITICAL_THRESHOLD=95
```

## Конфигурации для разных нагрузок

### Разработка и тестирование

**Характеристики:**

- 1-2 пользователя
- Небольшие документы
- Быстрый цикл разработки

**Рекомендуемые настройки:**

```env
# Кэширование
CACHE_QUERY_MAX_SIZE=200
CACHE_EMBEDDING_MAX_SIZE=500
CACHE_DEFAULT_TTL=1800

# Асинхронная обработка
ASYNC_MAX_WORKERS=2
ASYNC_LARGE_DOC_THRESHOLD=30000

# Мониторинг
PERFORMANCE_SLOW_THRESHOLD=3.0
PERFORMANCE_MEMORY_THRESHOLD=300
```

### Продуктивная среда

**Характеристики:**

- 5-10 пользователей
- Средние и большие документы
- Стабильная нагрузка

**Рекомендуемые настройки:**

```env
# Кэширование
CACHE_QUERY_MAX_SIZE=1000
CACHE_EMBEDDING_MAX_SIZE=2000
CACHE_DEFAULT_TTL=3600

# Асинхронная обработка
ASYNC_MAX_WORKERS=8
ASYNC_LARGE_DOC_THRESHOLD=100000

# Мониторинг
PERFORMANCE_SLOW_THRESHOLD=5.0
PERFORMANCE_MEMORY_THRESHOLD=1000
```

### Высокие нагрузки

**Характеристики:**

- 10+ пользователей
- Большие объемы документов
- Пиковые нагрузки

**Рекомендуемые настройки:**

```env
# Кэширование
CACHE_QUERY_MAX_SIZE=2000
CACHE_EMBEDDING_MAX_SIZE=5000
CACHE_DEFAULT_TTL=7200

# Асинхронная обработка
ASYNC_MAX_WORKERS=16
ASYNC_LARGE_DOC_THRESHOLD=50000

# Мониторинг
PERFORMANCE_SLOW_THRESHOLD=10.0
PERFORMANCE_MEMORY_THRESHOLD=2000

# Дополнительные оптимизации
ENABLE_EMBEDDING_PRECOMPUTATION=true
ENABLE_QUERY_RESULT_AGGREGATION=true
```

## Команды мониторинга

### Статистика производительности

```bash
# Общая статистика
docker-compose exec ai-agent poetry run python -m ai_agent.main performance --stats

# Медленные операции
docker-compose exec ai-agent poetry run python -m ai_agent.main performance --slow

# Статистика конкретной операции
docker-compose exec ai-agent poetry run python -m ai_agent.main performance --stats --operation search_similar_chunks
```

### Управление кэшем

```bash
# Статистика кэша
docker-compose exec ai-agent poetry run python -m ai_agent.main cache --stats

# Очистка кэша
docker-compose exec ai-agent poetry run python -m ai_agent.main cache --clear

# Очистка устаревших записей
docker-compose exec ai-agent poetry run python -m ai_agent.main cache --cleanup
```

### Системный статус

```bash
# Полный статус системы
docker-compose exec ai-agent poetry run python -m ai_agent.main status
```

## Оптимизация по типам операций

### Поиск по документам

**Проблема**: Медленный поиск релевантных чанков

**Решения:**

1. Увеличить размер embedding cache
2. Оптимизировать размер чанков
3. Использовать фильтрацию по категориям

```env
CACHE_EMBEDDING_MAX_SIZE=2000
CACHE_EMBEDDING_TTL=7200
```

### Загрузка документов

**Проблема**: Медленная обработка больших документов

**Решения:**

1. Включить асинхронную обработку
2. Увеличить количество воркеров
3. Оптимизировать чанкинг

```env
ASYNC_MAX_WORKERS=8
ASYNC_LARGE_DOC_THRESHOLD=50000
```

### Генерация ответов

**Проблема**: Медленная генерация ответов LLM

**Решения:**

1. Кэшировать частые запросы
2. Оптимизировать размер контекста
3. Использовать более быстрые модели для простых запросов

```env
CACHE_QUERY_MAX_SIZE=1000
CACHE_DEFAULT_TTL=3600
```

## Диагностика проблем производительности

### Высокое использование CPU

**Симптомы:**

- CPU > 90%
- Медленные ответы
- Таймауты операций

**Диагностика:**

```bash
# Проверить медленные операции
docker-compose exec ai-agent poetry run python -m ai_agent.main performance --slow

# Системные ресурсы
docker-compose exec ai-agent poetry run python -m ai_agent.main status
```

**Решения:**

1. Увеличить кэширование
2. Уменьшить количество воркеров
3. Оптимизировать размер чанков

### Высокое использование памяти

**Симптомы:**

- Память > 90%
- OutOfMemory ошибки
- Медленная работа системы

**Диагностика:**

```bash
# Статистика кэша
docker-compose exec ai-agent poetry run python -m ai_agent.main cache --stats

# Статистика асинхронной обработки
docker-compose exec ai-agent poetry run python -m ai_agent.main status
```

**Решения:**

1. Уменьшить размер кэша
2. Очистить кэш
3. Уменьшить количество воркеров

### Низкий коэффициент попаданий в кэш

**Симптомы:**

- Hit rate < 50%
- Повторные медленные запросы
- Высокая нагрузка на Ollama

**Решения:**

1. Увеличить размер кэша
2. Увеличить TTL
3. Проанализировать паттерны запросов

## Лучшие практики

### 1. Мониторинг

- Регулярно проверяйте статистику производительности
- Настройте алерты для критических метрик
- Ведите лог медленных операций

### 2. Кэширование

- Настройте размер кэша под вашу нагрузку
- Используйте разные TTL для разных типов данных
- Регулярно очищайте устаревшие записи

### 3. Ресурсы

- Мониторьте использование CPU и памяти
- Настройте количество воркеров под ваше железо
- Используйте SSD для хранения данных

### 4. Документы

- Оптимизируйте размер загружаемых документов
- Используйте категоризацию для эффективного поиска
- Регулярно очищайте неиспользуемые документы

## Заключение

Система оптимизации производительности AI Agent обеспечивает:

- **Автоматическую адаптацию** к различным нагрузкам
- **Эффективное использование ресурсов**
- **Масштабируемость** от разработки до продакшена
- **Детальный мониторинг** и диагностику

Следуя рекомендациям этого руководства, вы сможете достичь оптимальной производительности для вашего конкретного случая использования.
